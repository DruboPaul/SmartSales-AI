# ğŸ—ï¸ Enterprise Retail ETL Pipeline

> A production-grade batch ETL pipeline for daily retail transaction ingestion into BigQuery, orchestrated with Apache Airflow.

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8.1-017CEE?logo=apache-airflow)](https://airflow.apache.org/)
[![Google Cloud](https://img.shields.io/badge/Google%20Cloud-BigQuery-4285F4?logo=google-cloud)](https://cloud.google.com/bigquery)
[![Python](https://img.shields.io/badge/Python-3.11-3776AB?logo=python)](https://python.org)

---

## ğŸ“Š Architecture

```mermaid
flowchart LR
    subgraph Local["ğŸ“ Local Environment"]
        GEN["ğŸ”„ Data Generator<br/>(Python + Faker)"]
    end
    
    subgraph Airflow["âš™ï¸ Apache Airflow"]
        DAG["ğŸ“… Daily DAG<br/>(retail_batch_etl)"]
    end
    
    subgraph GCP["â˜ï¸ Google Cloud Platform"]
        GCS["ğŸª£ Cloud Storage<br/>(Raw Data Lake)"]
        STG["ğŸ“‹ Staging Table<br/>(stg_sales)"]
        FACT["â­ Fact Table<br/>(fact_transactions)"]
        VIEW["ğŸ“ˆ BI Views<br/>(daily_revenue)"]
    end
    
    GEN --> DAG
    DAG -->|Upload CSV| GCS
    GCS -->|Load| STG
    STG -->|Transform| FACT
    FACT --> VIEW
```

---

## âš¡ Pipeline Flow

| Step | Task ID | Description |
|------|---------|-------------|
| 1ï¸âƒ£ | `generate_local_csv` | Generate 1000+ synthetic retail transactions |
| 2ï¸âƒ£ | `upload_to_gcs` | Upload CSV to GCS bucket with date partitioning |
| 3ï¸âƒ£ | `load_to_bq_staging` | Load raw data to BigQuery staging table |
| 4ï¸âƒ£ | `transform_to_fact` | Transform & insert into star-schema fact table |

---

## ğŸ› ï¸ Tech Stack

- **Orchestration**: Apache Airflow 2.8
- **Data Warehouse**: Google BigQuery
- **Data Lake**: Google Cloud Storage
- **Containerization**: Docker + Docker Compose
- **Language**: Python 3.11

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose installed
- GCP Service Account with BigQuery & GCS permissions

### 1. Clone & Configure
```bash
# Copy environment template
cp .env.example .env

# Add your GCP credentials
mkdir credentials
cp /path/to/your/service-account.json credentials/
```

### 2. Start Airflow
```bash
# Initialize the database
docker-compose up airflow-init

# Start all services
docker-compose up -d
```

### 3. Access Airflow UI
Open [http://localhost:8080](http://localhost:8080)
- **Username**: `admin`
- **Password**: `admin`

### 4. Trigger the DAG
1. Enable the `retail_batch_etl` DAG
2. Click "Trigger DAG" to run manually

---

## ğŸ“ Project Structure

```
ProjectA_Batch_ETL/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ retail_batch_etl.py    # Main Airflow DAG
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_generator.py      # Synthetic data generator
â”‚   â””â”€â”€ bigquery_setup.sql     # BigQuery schema definitions
â”œâ”€â”€ logs/                       # Airflow logs
â”œâ”€â”€ plugins/                    # Custom Airflow plugins
â”œâ”€â”€ credentials/                # GCP service account (git-ignored)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š BigQuery Schema (Star Schema)

### Fact Table: `fact_transactions`
| Column | Type | Description |
|--------|------|-------------|
| `transaction_id` | STRING | Unique transaction ID |
| `store_id` | STRING | Store identifier |
| `product_id` | STRING | Product identifier |
| `category` | STRING | Product category |
| `price` | FLOAT64 | Transaction price |
| `quantity` | INT64 | Quantity sold |
| `transaction_time` | TIMESTAMP | When the sale occurred |
| `insertion_time` | TIMESTAMP | ETL processing time |

**Optimizations**: Partitioned by `transaction_time`, clustered by `category` and `store_id`.

---

## ğŸ“ˆ Sample BI Queries

```sql
-- Daily revenue by category
SELECT 
    DATE(transaction_time) as sale_date,
    category,
    SUM(price * quantity) as total_revenue
FROM `project.retail_analytics.fact_transactions`
GROUP BY 1, 2
ORDER BY 1 DESC, 3 DESC;
```

---

## ğŸ”§ Configuration

Edit the following in `dags/retail_batch_etl.py`:

```python
PROJECT_ID = "your-gcp-project-id"
BUCKET_NAME = "your-gcs-bucket"
DATASET_ID = "retail_analytics"
```

---

## ğŸ“„ License

MIT License - Feel free to use for your portfolio!
